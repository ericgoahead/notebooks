{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pandas is used for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data preparation (data loading, cleaning,convertion,exploration...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load data (train & test datasets) and append test dataset to train dataset ,making a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define 'columns' to be used to set dataframe's columns\n",
    "columns=['age','workclass','fnlwgt','education','education-num','marital-status','occupation','relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country','income']\n",
    "# Skip the first row. 'header' set to None as there is no feature names in the file. We use 'name=' to set dataframe's columns.\n",
    "df=pd.read_csv('adult.data',header=None,names=columns)\n",
    "print(df.columns)\n",
    "print(df.shape)\n",
    "df_test=pd.read_csv('adult.test',skiprows=[0],header=None,names=columns)\n",
    "df=df.append(df_test,ignore_index=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Strip white spaces and trailing dot for some of feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# strip white space and trailing dot from  for some columns!\n",
    "df['income']=df['income'].apply(lambda x: x.replace(' ','').replace('.',''))\n",
    "df['sex']=df['sex'].apply(lambda x: x.replace(' ',''))\n",
    "df['race']=df['race'].apply(lambda x: x.replace(' ',''))\n",
    "df['relationship']=df['relationship'].apply(lambda x: x.replace(' ',''))\n",
    "df['occupation']=df['occupation'].apply(lambda x: x.replace(' ',''))\n",
    "df['education']=df['education'].apply(lambda x: x.replace(' ',''))\n",
    "df['marital-status']=df['marital-status'].apply(lambda x: x.replace(' ',''))\n",
    "df['native-country']=df['native-country'].apply(lambda x: x.replace(' ',''))\n",
    "# add a new column of boolean type \n",
    "df['income_greater_50K']=df.income=='>50K'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table=pd.pivot_table(df,index=['occupation','sex'],values=['age'],aggfunc=[np.mean])\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['dumb'] = int(1)\n",
    "table=pd.pivot_table(df,index=['race'],columns=['education'],values=['dumb'],aggfunc=[np.sum],margins=True,fill_value=0)\n",
    "df.drop('dumb',axis=1,inplace=True)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['dumb'] = int(1)\n",
    "table=pd.pivot_table(df,index=['sex'],columns=['income'],values=['dumb'],aggfunc=[np.sum],margins=True)\n",
    "df.drop('dumb',axis=1,inplace=True)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table1=table['sum']['dumb']\n",
    "table1.loc['Female ratio']=table1.loc['Female']/table1.loc['All']\n",
    "print(\"Women percentage: {2} <=50K percentage :{0},>50K percentage :{1}\".\\\n",
    "      format(table1.loc['Female ratio','<=50K'],table1.loc['Female ratio','>50K'],table1.loc['Female ratio','All']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pie chart to display the proportion of data samples in each individual 'occupation' category. \n",
    "df['occupation'].value_counts().plot(kind='pie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(df[(df.age>=27)&(df.race=='Black')&(df['income_greater_50K']==True)].shape[0],df[(df.age>=27)&(df.race=='Black')].shape[0])\n",
    "print(df[(df.age>=27)&(df.race=='White')&(df['income_greater_50K']==True)].shape[0],df[(df.age>=27)&(df.race=='White')].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print df.describe(include=[np.object])\n",
    "df.isnull().describe()   # check to see if there are any NaN s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stacked bar chart to show  'income' breakdown per 'sex' \n",
    "a=df.groupby(['sex','income']).size()\n",
    "b=a.unstack()\n",
    "print(b, b.index,b.columns)\n",
    "#b.plot(kind='bar',stacked=True)\n",
    "b.plot(kind='bar',stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stacked bar chart to show  'sex' breakdown per 'occupation' \n",
    "a=df.groupby(['occupation','sex']).size()\n",
    "b=a.unstack()\n",
    "b.plot(kind='bar',stacked=True)\n",
    "# Stacked bar chart to show  'occupation' breakdown per 'sex' \n",
    "df.groupby(['sex','occupation']).size().unstack().plot(kind='barh',stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Histogram to display 'hours-per-week' distribution per 'sex'\n",
    "df.groupby(['sex'])['hours-per-week'].plot(kind='hist',legend=True,alpha=.5,bins=np.arange(start=0,stop=100,step=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Figure out  ratio of '>=50K' income in each gender \n",
    "a= df.groupby(['sex','income']).size()\n",
    "print(\"Ratio of women whose income is less than $50K: {}\".\\\n",
    "      format(a.loc['Female'].loc['<=50K'].astype('float') / a.loc['Female'].astype('float').sum()))\n",
    "print(\"Ratio of women whose income is greater than $50K: {}\".\\\n",
    "      format(a.loc['Female'].loc['>50K'].astype('float') / a.loc['Female'].astype('float').sum()))\n",
    "\n",
    "print(\"Ratio of men whose income is less than $50K: {}\".\\\n",
    "      format(a.loc['Male'].loc['<=50K'].astype('float') / a.loc['Male'].astype('float').sum()))\n",
    "print(\"Ratio of men whose income is greater than $50K: {}\".\\\n",
    "      format(a.loc['Male'].loc['>50K'].astype('float') / a.loc['Male'].astype('float').sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Drop `fnlwgt` feature column and convert all non-numerical features to numerical ones  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop('fnlwgt',axis=1,inplace=True)\n",
    "target=df['income_greater_50K'].astype('int')\n",
    "features=df.drop(['income','income_greater_50K'],axis=1)\n",
    "features.dtypes\n",
    "features_numeric=pd.get_dummies(features)\n",
    "features_numeric.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Split the single dataframe into a dataframe storing train data  and a dataframe storing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train_features= features_numeric.iloc[:32561]\n",
    "test_features= features_numeric.iloc[32561:]\n",
    "train_target= target.iloc[:32561]\n",
    "test_target= target.iloc[32561:]\n",
    "\"\"\"\n",
    "train_features= features_numeric.iloc[:]\n",
    "train_target= target.iloc[:]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Cross-validation (against train dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1a Cross validation on `SVC` classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "scaler=StandardScaler()\n",
    "clf = SVC(C=1.0,cache_size=300,kernel='rbf')\n",
    "pipeline= Pipeline([('transformer',scaler),('estimator',clf)])\n",
    "#pipeline= Pipeline([('estimator',clf)])\n",
    "#clf=SVC(kernel='poly',cache_size=500)\n",
    "scores= cross_val_score(pipeline,train_features.values,train_target.values,cv=5,n_jobs=-1,verbose=2)\n",
    "print(scores)\n",
    "print(\"Accuracy: %0.4f (+/- %0.4f)\" % (scores.mean(),scores.std()*2))\n",
    "\"\"\"\n",
    "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1b Cross validation on `Adaboost` classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#clf = RandomForestClassifier(n_estimators=500)\n",
    "#clf=DecisionTreeClassifier(max_depth=5)\n",
    "clf= AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),\n",
    "                         algorithm=\"SAMME\",\n",
    "                         n_estimators=400)\n",
    "scores= cross_val_score(clf,train_features.values,train_target.values,cv=8,n_jobs=-1,verbose=9)\n",
    "print(scores)\n",
    "print(\"Accuracy: %0.4f (+/- %0.4f)\" % (scores.mean(),scores.std()*2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1c Cross validation on `LogisticRegression` classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "scaler=StandardScaler()\n",
    "clf = LogisticRegression(solver='lbfgs')\n",
    "pipeline= Pipeline([('transformer',scaler),('estimator',clf)])\n",
    "#pipeline= Pipeline([('estimator',clf)])\n",
    "scores= cross_val_score(pipeline,train_features.values,train_target.values,cv=8,n_jobs=-1,verbose=2)\n",
    "print(scores)\n",
    "print(\"Accuracy: %0.4f (+/- %0.4f)\" % (scores.mean(),scores.std()*2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 3:Learn a classifier from train dataset and test it against test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1a Mini-batch gradient descend method:Train Logistic Regression classifier with `SGDClassifer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import random\n",
    "# Generator of series of lists each of which is at most 'n' in length\n",
    "def batches(l, n):\n",
    "    for i in xrange(0, len(l), n):\n",
    "        yield l[i:i+n]\n",
    "scaler=StandardScaler()\n",
    "# To normalize the input features by 'fit' first and 'transform' later. 'scalerX' stores normalized feature inputs of the train data set.\n",
    "scaler.fit(train_features.values)\n",
    "scalerX =scaler.transform(train_features.values)\n",
    "# Define a logistic regression classifier\n",
    "clf = SGDClassifier(loss='log') # shuffle=True by default\n",
    "shuffledRange = range(len(train_target.values))\n",
    "# 'epochs': number of passes of the trian data set for the training\n",
    "# 'batch_size': Mini-batch size\n",
    "epochs = 50\n",
    "batch_size=4000\n",
    "# Mini-batch gradient descend optimization loop\n",
    "for n in range(epochs):\n",
    "    random.shuffle(shuffledRange)\n",
    "    shuffledX = [scalerX[i] for i in shuffledRange]\n",
    "    shuffledY = [train_target.values[i] for i in shuffledRange]\n",
    "    for batch in batches(range(len(shuffledX)), batch_size):\n",
    "        clf.partial_fit(shuffledX[batch[0]:batch[-1]+1], shuffledY[batch[0]:batch[-1]+1], classes=np.unique(train_target.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1b Stochastic gradient descend method :Train Logistic Regression classifier with `SGDClassifer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(train_features.values)\n",
    "scalerX =scaler.transform(train_features.values)\n",
    "clf = SGDClassifier(loss='log',max_iter=10,shuffle=True) # shuffle=True is useless here\n",
    "clf.fit(scalerX, train_target.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1c Train a `LogisticRegression` classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(train_features.values)\n",
    "scalerX =scaler.transform(train_features.values)\n",
    "clf = LogisticRegression(solver='lbfgs')\n",
    "clf.fit(scalerX, train_target.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Test the classifier on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "results_predicted = clf.predict(scaler.transform(test_features.values))\n",
    "print(classification_report(test_target.values, results_predicted))\n",
    "compare = results_predicted!= test_target.values\n",
    "print(\"Test error ratio: {}\".format(float(np.sum(compare.astype('int')))/float(np.size(compare.astype('int')))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
